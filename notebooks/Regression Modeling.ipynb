{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression modeling\n",
    "\n",
    "A general, primary goal of many statistical data analysis tasks is to relate the influence of one variable on another. For example, we may wish to know how different medical interventions influence the incidence or duration of disease, or perhaps a how baseball player's performance varies as a function of age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from scipy.optimize import fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([2.2, 4.3, 5.1, 5.8, 6.4, 8.0])\n",
    "y = np.array([0.4, 10.1, 14.0, 10.9, 15.4, 18.5])\n",
    "plt.plot(x,y,'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a model to characterize the relationship between $X$ and $Y$, recognizing that additional factors other than $X$ (the ones we have measured or are interested in) may influence the response variable $Y$.\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$y_i = f(x_i) + \\epsilon_i$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $f$ is some function, for example a linear function:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $\\epsilon_i$ accounts for the difference between the observed response $y_i$ and its prediction from the model $\\hat{y_i} = \\beta_0 + \\beta_1 x_i$. This is sometimes referred to as **process uncertainty**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to select $\\beta_0, \\beta_1$ so that the difference between the predictions and the observations is zero, but this is not usually possible. Instead, we choose a reasonable criterion: ***the smallest sum of the squared differences between $\\hat{y}$ and $y$***.\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$R^2 = \\sum_i (y_i - [\\beta_0 + \\beta_1 x_i])^2 = \\sum_i \\epsilon_i^2 $$  \n",
    "</div>\n",
    "\n",
    "Squaring serves two purposes: (1) to prevent positive and negative values from cancelling each other out and (2) to strongly penalize large deviations. Whether the latter is a good thing or not depends on the goals of the analysis.\n",
    "\n",
    "In other words, we will select the parameters that minimize the squared error of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_of_squares = lambda theta, x, y: np.sum((y - theta[0] - theta[1]*x) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_of_squares([0,1],x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b0,b1 = fmin(sum_of_squares, [0,1], args=(x,y))\n",
    "b0,b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([0,10], [b0, b0+b1*10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([0,10], [b0, b0+b1*10])\n",
    "for xi, yi in zip(x,y):\n",
    "    plt.plot([xi]*2, [yi, b0+b1*xi], 'k:')\n",
    "plt.xlim(2, 9); plt.ylim(0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing the sum of squares is not the only criterion we can use; it is just a very popular (and successful) one. For example, we can try to minimize the sum of absolute differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_of_absval = lambda theta, x, y: np.sum(np.abs(y - theta[0] - theta[1]*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b0,b1 = fmin(sum_of_absval, [0,1], args=(x,y))\n",
    "print('\\nintercept: {0:.2}, slope: {1:.2}'.format(b0,b1))\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([0,10], [b0, b0+b1*10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not restricted to a straight-line regression model; we can represent a curved relationship between our variables by introducing **polynomial** terms. For example, a cubic model:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon_i$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_squares_quad = lambda theta, x, y: np.sum((y - theta[0] - theta[1]*x - theta[2]*(x**2)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b0,b1,b2 = fmin(sum_squares_quad, [1,1,-1], args=(x,y))\n",
    "print('\\nintercept: {0:.2}, x: {1:.2}, x2: {2:.2}'.format(b0,b1,b2))\n",
    "plt.plot(x, y, 'ro')\n",
    "xvals = np.linspace(0, 10, 100)\n",
    "plt.plot(xvals, b0 + b1*xvals + b2*(xvals**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although polynomial model characterizes a nonlinear relationship, it is a linear problem in terms of estimation. That is, the regression model $f(y | x)$ is linear in the parameters.\n",
    "\n",
    "For some data, it may be reasonable to consider polynomials of order>2. For example, consider the relationship between the number of home runs a baseball player hits and the number of runs batted in (RBI) they accumulate; clearly, the relationship is positive, but we may not expect a linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_squares_cubic = lambda theta, x, y: np.sum((y - theta[0] - theta[1]*x - theta[2]*(x**2) \n",
    "                                  - theta[3]*(x**3)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bb = pd.read_csv(\"../data/baseball.csv\", index_col=0)\n",
    "plt.plot(bb.hr, bb.rbi, 'r.')\n",
    "b0,b1,b2,b3 = fmin(sum_squares_cubic, [0,1,-1,0], args=(bb.hr, bb.rbi))\n",
    "xvals = np.arange(40)\n",
    "plt.plot(xvals, b0 + b1*xvals + b2*(xvals**2) + b3*(xvals**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Polynomial function\n",
    "\n",
    "Write a function that specifies a polynomial of arbitrary degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we need not fit least squares models by hand because they are implemented generally in packages such as [`scikit-learn`](http://scikit-learn.org/) and [`statsmodels`](https://github.com/statsmodels/statsmodels/). For example, `scikit-learn` package implements least squares models in its `LinearRegression` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "straight_line = linear_model.LinearRegression()\n",
    "straight_line.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "straight_line.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'ro')\n",
    "plt.plot(x, straight_line.predict(x[:, np.newaxis]), color='blue',\n",
    "         linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more general regression model building, its helpful to use a tool for describing statistical models, called `patsy`. With `patsy`, it is easy to specify the desired combinations of variables for any particular analysis, using an \"R-like\" syntax. `patsy` parses the formula string, and uses it to construct the approriate *design matrix* for the model.\n",
    "\n",
    "For example, the quadratic model specified by hand above can be coded as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "\n",
    "X = dmatrix('x + I(x**2)')\n",
    "np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dmatrix` function returns the design matrix, which can be passed directly to the `LinearRegression` fitting method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly_line = linear_model.LinearRegression(fit_intercept=False)\n",
    "poly_line.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poly_line.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'ro')\n",
    "plt.plot(x, poly_line.predict(X), color='blue',\n",
    "         linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "How do we choose among competing models for a given dataset? More parameters are not necessarily better, from the standpoint of model fit. For example, fitting a 9-th order polynomial to the sample data from the above example certainly results in an overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_poly(params, data):\n",
    "        x = np.c_[[data**i for i in range(len(params))]]\n",
    "        return np.dot(params, x)\n",
    "    \n",
    "sum_squares_poly = lambda theta, x, y: np.sum((y - calc_poly(theta, x)) ** 2)\n",
    "betas = fmin(sum_squares_poly, np.zeros(10), args=(x,y), maxiter=1e6)\n",
    "plt.plot(x, y, 'ro')\n",
    "xvals = np.linspace(0, max(x), 100)\n",
    "plt.plot(xvals, calc_poly(betas, xvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach is to use an information-theoretic criterion to select the most appropriate model. For example **Akaike's Information Criterion (AIC)** balances the fit of the model (in terms of the likelihood) with the number of parameters required to achieve that fit. We can easily calculate AIC as:\n",
    "\n",
    "$$AIC = n \\log(\\hat{\\sigma}^2) + 2p$$\n",
    "\n",
    "where $p$ is the number of parameters in the model and $\\hat{\\sigma}^2 = RSS/(n-p-1)$.\n",
    "\n",
    "Notice that as the number of parameters increase, the residual sum of squares goes down, but the second term (a penalty) increases.\n",
    "\n",
    "To apply AIC to model selection, we choose the model that has the **lowest** AIC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(x)\n",
    "\n",
    "aic = lambda rss, p, n: n * np.log(rss/(n-p-1)) + 2*p\n",
    "\n",
    "RSS1 = sum_of_squares(fmin(sum_of_squares, [0,1], args=(x,y)), x, y)\n",
    "RSS2 = sum_squares_quad(fmin(sum_squares_quad, [1,1,-1], args=(x,y)), x, y)\n",
    "\n",
    "print('\\nModel 1: {0}\\nModel 2: {1}'.format(aic(RSS1, 2, n), aic(RSS2, 3, n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, on the basis of \"information distance\", we would select the 2-parameter (linear) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Fitting a line to the relationship between two variables using the least squares approach is sensible when the variable we are trying to predict is continuous, but what about when the data are dichotomous?\n",
    "\n",
    "- male/female\n",
    "- pass/fail\n",
    "- died/survived\n",
    "\n",
    "Let's consider the problem of predicting survival in the Titanic disaster, based on our available information. For example, lets say that we want to predict survival as a function of the fare paid for the journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_excel(\"../data/titanic.xls\", \"titanic\")\n",
    "titanic.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jitter = np.random.normal(scale=0.02, size=len(titanic))\n",
    "plt.scatter(np.log(titanic.fare), titanic.survived + jitter, alpha=0.3)\n",
    "plt.yticks([0,1])\n",
    "plt.ylabel(\"survived\")\n",
    "plt.xlabel(\"log(fare)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have added random jitter on the y-axis to help visualize the density of the points, and have plotted fare on the log scale.\n",
    "\n",
    "Clearly, fitting a line through this data makes little sense, for several reasons. First, for most values of the predictor variable, the line would predict values that are not zero or one. Second, it would seem odd to choose least squares (or similar) as a criterion for selecting the best line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.log(titanic.fare[titanic.fare>0])\n",
    "y = titanic.survived[titanic.fare>0]\n",
    "betas_titanic = fmin(sum_of_squares, [1,1], args=(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jitter = np.random.normal(scale=0.02, size=len(titanic))\n",
    "plt.scatter(np.log(titanic.fare), titanic.survived + jitter, alpha=0.3)\n",
    "plt.yticks([0,1])\n",
    "plt.ylabel(\"survived\")\n",
    "plt.xlabel(\"log(fare)\")\n",
    "plt.plot([0,7], [betas_titanic[0], betas_titanic[0] + betas_titanic[1]*7.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at this data, we can see that for most values of `fare`, there are some individuals that survived and some that did not. However, notice that the cloud of points is denser on the \"survived\" (`y=1`) side for larger values of fare than on the \"died\" (`y=0`) side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic model\n",
    "\n",
    "Rather than model the binary outcome explicitly, it makes sense instead to model the *probability* of death or survival in a **stochastic** model. Probabilities are measured on a continuous [0,1] scale, which may be more amenable for prediction using a regression line. We need to consider a different probability model for this exerciese however; let's consider the **Bernoulli** distribution as a generative model for our data:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$f(y|p) = p^{y} (1-p)^{1-y}$$ \n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $y = \\{0,1\\}$ and $p \\in [0,1]$. So, this model predicts whether $y$ is zero or one as a function of the probability $p$. Notice that when $y=1$, the $1-p$ term disappears, and when $y=0$, the $p$ term disappears.\n",
    "\n",
    "So, the model we want to fit should look something like this:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$p_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since $p$ is constrained to be between zero and one, it is easy to see where a linear (or polynomial) model might predict values outside of this range. We can modify this model sligtly by using a **link function** to transform the probability to have an unbounded range on a new scale. Specifically, we can use a **logit transformation** as our link function:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$\\text{logit}(p) = \\log\\left[\\frac{p}{1-p}\\right] = x$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of $p/(1-p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logit = lambda p: np.log(p/(1.-p))\n",
    "unit_interval = np.linspace(0,1)\n",
    "plt.plot(unit_interval/(1-unit_interval), unit_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the logit function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(logit(unit_interval), unit_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse of the logit transformation is:\n",
    "\n",
    "<div style=\"font-size: 150%;\">  \n",
    "$$p = \\frac{1}{1 + \\exp(-x)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "invlogit = lambda x: 1. / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now our model is:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$\\text{logit}(p_i) = \\beta_0 + \\beta_1 x_i + \\epsilon_i$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit this model using maximum likelihood. Our likelihood, again based on the Bernoulli model is:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$L(y|p) = \\prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which, on the log scale is:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$l(y|p) = \\sum_{i=1}^n y_i \\log(p_i) + (1-y_i)\\log(1-p_i)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily implement this in Python, keeping in mind that `fmin` minimizes, rather than maximizes functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_like(theta, x, y):\n",
    "    \n",
    "    p = invlogit(theta[0] + theta[1] * x)\n",
    "    \n",
    "    # Return negative of log-likelihood\n",
    "    return -np.sum(y * np.log(p) + (1-y) * np.log(1 - p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove null values from variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = titanic[titanic.fare.notnull()][['fare', 'survived']].values.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b0, b1 = fmin(logistic_like, [0.5,0], args=(x,y))\n",
    "b0, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jitter = np.random.normal(scale=0.01, size=len(x))\n",
    "plt.plot(x, y+jitter, 'r.', alpha=0.3)\n",
    "plt.yticks([0,.25,.5,.75,1])\n",
    "xvals = np.linspace(0, 600)\n",
    "plt.plot(xvals, invlogit(b0+b1*xvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with our least squares model, we can easily fit logistic regression models in `scikit-learn`, in this case using the `LogisticRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(x[:, np.newaxis], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: multivariate logistic regression\n",
    "\n",
    "Which other variables might be relevant for predicting the probability of surviving the Titanic? Generalize the model likelihood to include 2 or 3 other covariates from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
