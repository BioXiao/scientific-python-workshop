{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model checking and diagnostics\n",
    "\n",
    "## Convergence Diagnostics\n",
    "\n",
    "Valid inferences from sequences of MCMC samples are based on the\n",
    "assumption that the samples are derived from the true posterior\n",
    "distribution of interest. Theory guarantees this condition as the number\n",
    "of iterations approaches infinity. It is important, therefore, to\n",
    "determine the **minimum number of samples** required to ensure a reasonable\n",
    "approximation to the target posterior density. Unfortunately, no\n",
    "universal threshold exists across all problems, so convergence must be\n",
    "assessed independently each time MCMC estimation is performed. The\n",
    "procedures for verifying convergence are collectively known as\n",
    "*convergence diagnostics*.\n",
    "\n",
    "One approach to analyzing convergence is **analytical**, whereby the\n",
    "variance of the sample at different sections of the chain are compared\n",
    "to that of the limiting distribution. These methods use distance metrics\n",
    "to analyze convergence, or place theoretical bounds on the sample\n",
    "variance, and though they are promising, they are generally difficult to\n",
    "use and are not prominent in the MCMC literature. More common is a\n",
    "**statistical** approach to assessing convergence. With this approach,\n",
    "rather than considering the properties of the theoretical target\n",
    "distribution, only the statistical properties of the observed chain are\n",
    "analyzed. Reliance on the sample alone restricts such convergence\n",
    "criteria to **heuristics**. As a result, convergence cannot be guaranteed.\n",
    "Although evidence for lack of convergence using statistical convergence\n",
    "diagnostics will correctly imply lack of convergence in the chain, the\n",
    "absence of such evidence will not *guarantee* convergence in the chain.\n",
    "Nevertheless, negative results for one or more criteria may provide some\n",
    "measure of assurance to users that their sample will provide valid\n",
    "inferences.\n",
    "\n",
    "For most simple models, convergence will occur quickly, sometimes within\n",
    "a the first several hundred iterations, after which all remaining\n",
    "samples of the chain may be used to calculate posterior quantities. For\n",
    "more complex models, convergence requires a significantly longer burn-in\n",
    "period; sometimes orders of magnitude more samples are needed.\n",
    "Frequently, lack of convergence will be caused by **poor mixing**. \n",
    "Recall that *mixing* refers to the degree to which the Markov\n",
    "chain explores the support of the posterior distribution. Poor mixing\n",
    "may stem from inappropriate proposals (if one is using the\n",
    "Metropolis-Hastings sampler) or from attempting to estimate models with\n",
    "highly correlated variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pymc.examples import gelman_bioassay\n",
    "from pymc import MCMC, Matplot, Metropolis\n",
    "import seaborn as sns; sns.set_context('notebook')\n",
    "\n",
    "M = MCMC(gelman_bioassay)\n",
    "M.use_step_method(Metropolis, M.alpha, scale=0.001)\n",
    "M.sample(1000, tune_interval=1000)\n",
    "Matplot.plot(M.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informal Methods\n",
    "\n",
    "The most straightforward approach for assessing convergence is based on\n",
    "simply **plotting and inspecting traces and histograms** of the observed\n",
    "MCMC sample. If the trace of values for each of the stochastics exhibits\n",
    "asymptotic behavior over the last $m$ iterations, this may be\n",
    "satisfactory evidence for convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = MCMC(gelman_bioassay)\n",
    "M.sample(10000, burn=5000)\n",
    "Matplot.plot(M.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar approach involves\n",
    "plotting a histogram for every set of $k$ iterations (perhaps 50-100)\n",
    "beyond some burn in threshold $n$; if the histograms are not visibly\n",
    "different among the sample intervals, this may be considered some evidence for\n",
    "convergence. Note that such diagnostics should be carried out for each\n",
    "stochastic estimated by the MCMC algorithm, because convergent behavior\n",
    "by one variable does not imply evidence for convergence for other\n",
    "variables in the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14,6))\n",
    "axes = axes.ravel()\n",
    "for i in range(10):\n",
    "    axes[i].hist(M.beta.trace()[500*i:500*(i+1)])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extension of this approach can be taken\n",
    "when multiple parallel chains are run, rather than just a single, long\n",
    "chain. In this case, the final values of $c$ chains run for $n$\n",
    "iterations are plotted in a histogram; just as above, this is repeated\n",
    "every $k$ iterations thereafter, and the histograms of the endpoints are\n",
    "plotted again and compared to the previous histogram. This is repeated\n",
    "until consecutive histograms are indistinguishable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another *ad hoc* method for detecting lack of convergence is to examine\n",
    "the traces of several MCMC chains initialized with different starting\n",
    "values. Overlaying these traces on the same set of axes should (if\n",
    "convergence has occurred) show each chain tending toward the same\n",
    "equilibrium value, with approximately the same variance. Recall that the\n",
    "tendency for some Markov chains to converge to the true (unknown) value\n",
    "from diverse initial values is called *ergodicity*. This property is\n",
    "guaranteed by the reversible chains constructed using MCMC, and should\n",
    "be observable using this technique. Again, however, this approach is\n",
    "only a heuristic method, and cannot always detect lack of convergence,\n",
    "even though chains may appear ergodic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc.examples import disaster_model\n",
    "\n",
    "M = MCMC(disaster_model)\n",
    "M.early_mean.set_value(0.5)\n",
    "M.sample(1000)\n",
    "M.early_mean.set_value(5)\n",
    "M.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(M.early_mean.trace(chain=0)[:200], 'r--')\n",
    "plt.plot(M.early_mean.trace(chain=1)[:200], 'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A principal reason that evidence from informal techniques cannot\n",
    "guarantee convergence is a phenomenon called ***metastability***. Chains may\n",
    "appear to have converged to the true equilibrium value, displaying\n",
    "excellent qualities by any of the methods described above. However,\n",
    "after some period of stability around this value, the chain may suddenly\n",
    "move to another region of the parameter space. This period\n",
    "of metastability can sometimes be very long, and therefore escape\n",
    "detection by these convergence diagnostics. Unfortunately, there is no\n",
    "statistical technique available for detecting metastability.\n",
    "\n",
    "### Formal Methods\n",
    "\n",
    "Along with the *ad hoc* techniques described above, a number of more\n",
    "formal methods exist which are prevalent in the literature. These are\n",
    "considered more formal because they are based on existing statistical\n",
    "methods, such as time series analysis.\n",
    "\n",
    "PyMC currently includes three formal convergence diagnostic methods. The\n",
    "first, proposed by [Geweke (1992)](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1177011446), is a time-series approach that\n",
    "compares the mean and variance of segments from the beginning and end of\n",
    "a single chain.\n",
    "\n",
    "$$z = \\frac{\\bar{\\theta}_a - \\bar{\\theta}_b}{\\sqrt{S_a(0) + S_b(0)}}$$\n",
    "\n",
    "where $a$ is the early interval and $b$ the late interval, and $S_i(0)$ is the spectral density estimate at zero frequency for chain segment $i$. If the\n",
    "z-scores (theoretically distributed as standard normal variates) of\n",
    "these two segments are similar, it can provide evidence for convergence.\n",
    "PyMC calculates z-scores of the difference between various initial\n",
    "segments along the chain, and the last 50% of the remaining chain. If\n",
    "the chain has converged, the majority of points should fall within 2\n",
    "standard deviations of zero.\n",
    "\n",
    "In PyMC, diagnostic z-scores can be obtained by calling the `geweke` function. It\n",
    "accepts either (1) a single trace, (2) a Node or Stochastic object, or\n",
    "(4) an entire Model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc import geweke\n",
    "\n",
    "M = MCMC(gelman_bioassay)\n",
    "M.sample(5000)\n",
    "d = geweke(M.beta, intervals=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Matplot.geweke_plot(d, 'alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments expected are the following:\n",
    "\n",
    "-   `pymc_object`: The object that is or contains the output trace(s).\n",
    "\n",
    "-   `first` (optional): First portion of chain to be used in Geweke\n",
    "    diagnostic. Defaults to 0.1 (*i.e.* first 10% of chain).\n",
    "\n",
    "-   `last` (optional): Last portion of chain to be used in Geweke\n",
    "    diagnostic. Defaults to 0.5 (*i.e.* last 50% of chain).\n",
    "\n",
    "-   `intervals` (optional): Number of sub-chains to analyze. Defaults to\n",
    "    20.\n",
    "\n",
    "The resulting scores are best interpreted graphically, using the\n",
    "`geweke_plot` function. This displays the scores in series, in relation\n",
    "to the 2 standard deviation boundaries around zero. Hence, it is easy to\n",
    "see departures from the standard normal assumption.\n",
    "\n",
    "The second diagnostic provided by PyMC is the [Raftery and Lewis (1992)](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1177011143)\n",
    "procedure. This approach estimates the number of iterations required to\n",
    "reach convergence, along with the number of burn-in samples to be\n",
    "discarded and the appropriate thinning interval. A separate estimate of\n",
    "both quantities can be obtained for each variable in a given model.\n",
    "\n",
    "As the criterion for determining convergence, the Raftery and Lewis\n",
    "approach uses the accuracy of estimation of a user-specified quantile.\n",
    "For example, we may want to estimate the quantile $q=0.975$ to within\n",
    "$r=0.005$ with probability $s=0.95$. In other words,\n",
    "\n",
    "$$Pr(|\\hat{q}-q| \\le r) = s$$\n",
    "\n",
    "From any sample of $\\theta$, one can construct a binary chain:\n",
    "\n",
    "$$Z^{(j)} = I(\\theta^{(j)} \\le u_q)$$\n",
    "\n",
    "where $u_q$ is the quantile value and $I$ is the indicator function.\n",
    "While $\\{\\theta^{(j)}\\}$ is a Markov chain, $\\{Z^{(j)}\\}$ is not\n",
    "necessarily so. In any case, the serial dependency among $Z^{(j)}$\n",
    "decreases as the thinning interval $k$ increases. A value of $k$ is\n",
    "chosen to be the smallest value such that the first order Markov chain\n",
    "is preferable to the second order Markov chain.\n",
    "\n",
    "This thinned sample is used to determine number of burn-in samples. This\n",
    "is done by comparing the remaining samples from burn-in intervals of\n",
    "increasing length to the limiting distribution of the chain. An\n",
    "appropriate value is one for which the truncated sample's distribution\n",
    "is within $\\epsilon$ (arbitrarily small) of the limiting distribution.\n",
    "Estimates for sample size tend to be conservative.\n",
    "\n",
    "This diagnostic is best used on a short pilot run of a particular model,\n",
    "and the results used to parameterize a subsequent sample that is to be\n",
    "used for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc import raftery_lewis\n",
    "\n",
    "M = MCMC(gelman_bioassay)\n",
    "M.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raftery_lewis(M.alpha, q=0.025, r=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments are:\n",
    "\n",
    "-   `pymc_object`: The object that contains the Geweke scores. Can be a\n",
    "    list (one set) or a dictionary (multiple sets).\n",
    "\n",
    "-   `q`: Desired quantile to be estimated.\n",
    "\n",
    "-   `r`: Desired accuracy for quantile.\n",
    "\n",
    "-   `s` (optional): Probability of attaining the requested accuracy\n",
    "    (defaults to 0.95).\n",
    "\n",
    "-   `epsilon` (optional) : Half width of the tolerance interval required\n",
    "    for the q-quantile (defaults to 0.001).\n",
    "\n",
    "The third convergence diagnostic provided by PyMC is the Gelman-Rubin\n",
    "statistic [Gelman and Rubin (1992)](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1177011136). This diagnostic uses multiple chains to\n",
    "check for lack of convergence, and is based on the notion that if\n",
    "multiple chains have converged, by definition they should appear very\n",
    "similar to one another; if not, one or more of the chains has failed to\n",
    "converge.\n",
    "\n",
    "The Gelman-Rubin diagnostic uses an analysis of variance approach to\n",
    "assessing convergence. That is, it calculates both the between-chain\n",
    "varaince (B) and within-chain varaince (W), and assesses whether they\n",
    "are different enough to worry about convergence. Assuming $m$ chains,\n",
    "each of length $n$, quantities are calculated by:\n",
    "\n",
    "$$\\begin{align}B &= \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{.j} - \\bar{\\theta}_{..})^2 \\\\\n",
    "W &= \\frac{1}{m} \\sum_{j=1}^m \\left[ \\frac{1}{n-1} \\sum_{i=1}^n (\\theta_{ij} - \\bar{\\theta}_{.j})^2 \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "for each scalar estimand $\\theta$. Using these values, an estimate of\n",
    "the marginal posterior variance of $\\theta$ can be calculated:\n",
    "\n",
    "$$\\hat{\\text{Var}}(\\theta | y) = \\frac{n-1}{n} W + \\frac{1}{n} B$$\n",
    "\n",
    "Assuming $\\theta$ was initialized to arbitrary starting points in each\n",
    "chain, this quantity will overestimate the true marginal posterior\n",
    "variance. At the same time, $W$ will tend to underestimate the\n",
    "within-chain variance early in the sampling run. However, in the limit\n",
    "as $n \\rightarrow \n",
    "\\infty$, both quantities will converge to the true variance of $\\theta$.\n",
    "In light of this, the Gelman-Rubin statistic monitors convergence using\n",
    "the ratio:\n",
    "\n",
    "$$\\hat{R} = \\sqrt{\\frac{\\hat{\\text{Var}}(\\theta | y)}{W}}$$\n",
    "\n",
    "This is called the potential scale reduction, since it is an estimate of\n",
    "the potential reduction in the scale of $\\theta$ as the number of\n",
    "simulations tends to infinity. In practice, we look for values of\n",
    "$\\hat{R}$ close to one (say, less than 1.1) to be confident that a\n",
    "particular estimand has converged. In PyMC, the function\n",
    "`gelman_rubin` will calculate $\\hat{R}$ for each stochastic node in\n",
    "the passed model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc import gelman_rubin\n",
    "\n",
    "M = MCMC(gelman_bioassay)\n",
    "M.sample(1000)\n",
    "M.sample(1000)\n",
    "M.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gelman_rubin(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best results, each chain should be initialized to highly\n",
    "dispersed starting values for each stochastic node.\n",
    "\n",
    "By default, when calling the `summary_plot` function using nodes with\n",
    "multiple chains, the $\\hat{R}$ values will be plotted alongside the\n",
    "posterior intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "Matplot.summary_plot(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit\n",
    "\n",
    "Checking for model convergence is only the first step in the evaluation\n",
    "of MCMC model outputs. It is possible for an entirely unsuitable model\n",
    "to converge, so additional steps are needed to ensure that the estimated\n",
    "model adequately fits the data. One intuitive way of evaluating model\n",
    "fit is to compare model predictions with the observations used to fit\n",
    "the model. In other words, the fitted model can be used to simulate\n",
    "data, and the distribution of the simulated data should resemble the\n",
    "distribution of the actual data.\n",
    "\n",
    "Fortunately, simulating data from the model is a natural component of\n",
    "the Bayesian modelling framework. Recall, from the discussion on\n",
    "imputation of missing data, the posterior predictive distribution:\n",
    "\n",
    "$$p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) f(\\theta|y) d\\theta$$\n",
    "\n",
    "Here, $\\tilde{y}$ represents some hypothetical new data that would be\n",
    "expected, taking into account the posterior uncertainty in the model\n",
    "parameters. Sampling from the posterior predictive distribution is easy\n",
    "in PyMC. The code looks identical to the corresponding data stochastic,\n",
    "with two modifications: (1) the node should be specified as\n",
    "deterministic and (2) the statistical likelihoods should be replaced by\n",
    "random number generators. Consider the `gelman_bioassay` example, \n",
    "where deaths are modeled as a binomial random variable for which\n",
    "the probability of death is a logit-linear function of the dose of a\n",
    "particular drug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc import Normal, Binomial, deterministic, invlogit\n",
    "\n",
    "n = [5]*4 \n",
    "dose = [-.86,-.3,-.05,.73] \n",
    "x = [0,1,3,5]\n",
    "\n",
    "alpha = Normal('alpha', mu=0.0, tau=0.01) \n",
    "beta = Normal('beta', mu=0.0, tau=0.01)\n",
    "\n",
    "@deterministic \n",
    "def theta(a=alpha, b=beta, d=dose):\n",
    "    \"\"\"theta = inv_logit(a+b)\"\"\" \n",
    "    return invlogit(a+b*d)\n",
    "\n",
    "# deaths ~ binomial(n, p)\n",
    "deaths = Binomial('deaths', n=n, p=theta, value=x, observed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior predictive distribution of deaths uses the same functional\n",
    "form as the data likelihood, in this case a binomial stochastic. Here is\n",
    "the corresponding sample from the posterior predictive distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deaths_sim = Binomial('deaths_sim', n=n, p=theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the observed stochastic `Binomial` has been replaced\n",
    "with a stochastic node that is identical in every respect to \\`deaths\\`,\n",
    "except that its values are not fixed to be the observed data -- they are\n",
    "left to vary according to the values of the fitted parameters.\n",
    "\n",
    "The degree to which simulated data correspond to observations can be\n",
    "evaluated in at least two ways. First, these quantities can simply be\n",
    "compared visually. This allows for a qualitative comparison of\n",
    "model-based replicates and observations. If there is poor fit, the true\n",
    "value of the data may appear in the tails of the histogram of replicated\n",
    "data, while a good fit will tend to show the true data in\n",
    "high-probability regions of the posterior predictive distribution.\n",
    "The Matplot package in PyMC provides an easy way of producing such\n",
    "plots, via the `gof_plot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M_gof = MCMC([alpha, beta, theta, deaths, deaths_sim])\n",
    "M_gof.sample(2000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Matplot.gof_plot(deaths_sim.trace(), x, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second approach for evaluating goodness of fit using samples from the\n",
    "posterior predictive distribution involves the use of a statistical\n",
    "criterion. For example, the Bayesian p-value [(Gelman et al. 1996)](http://www.cs.princeton.edu/courses/archive/fall09/cos597A/papers/GelmanMengStern1996.pdf) uses a\n",
    "discrepancy measure that quantifies the difference between data\n",
    "(observed or simulated) and the expected value, conditional on some\n",
    "model. One such discrepancy measure is the Freeman-Tukey statistic\n",
    "[(Brooks et al. 2000)](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1009213003):\n",
    "\n",
    "$$D(x|\\theta) = \\sum_j (\\sqrt{x_j}-\\sqrt{e_j})^2,$$\n",
    "\n",
    "where the $x_j$ are data and $e_j$ are the corresponding expected\n",
    "values, based on the model. Model fit is assessed by comparing the\n",
    "discrepancies from observed data to those from simulated data. On\n",
    "average, we expect the difference between them to be zero; hence, the\n",
    "Bayesian *p* value is simply the proportion of simulated discrepancies\n",
    "that are larger than their corresponding observed discrepancies:\n",
    "\n",
    "$$p = Pr[ D(x_{\\text{sim}}|\\theta) > D(x_{\\text{obs}}|\\theta) ]$$\n",
    "\n",
    "If $p$ is very large (e.g. $>0.975$) or very small (e.g. $\\lt 0.025$) this\n",
    "implies that the model is not consistent with the data, and thus is\n",
    "evidence of lack of fit. Graphically, data and simulated discrepancies\n",
    "plotted together should be clustered along a 45 degree line passing\n",
    "through the origin.\n",
    "\n",
    "The `discrepancy` function in the `diagnostics` package can be used to\n",
    "generate discrepancy statistics from arrays of data, simulated values,\n",
    "and expected values:\n",
    "\n",
    "    D = pymc.discrepancy(x, x_sim, x_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc import discrepancy\n",
    "\n",
    "expected = theta.trace\n",
    "d = discrepancy(x, deaths_sim, (theta.trace() * n).T)\n",
    "d[0][:10], d[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset of size $n$ and an MCMC chain of length $r$, this implies\n",
    "that `x` is size `(n,)`, `x_sim` is size `(r,n)` and `x_exp` is either\n",
    "size `(r,)` or `(r,n)`. A call to this function returns two arrays of\n",
    "discrepancy values (simulated and observed), which can be passed to the\n",
    "`discrepancy_plot` function in the \\`Matplot\\` module to generate a\n",
    "scatter plot, and if desired, a *p* value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Matplot.discrepancy_plot(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Meta-analysis of beta blocker effectiveness\n",
    "\n",
    "Carlin (1992) considers a Bayesian approach to meta-analysis, and includes the following examples of 22 trials of beta-blockers to prevent mortality after myocardial infarction.\n",
    "\n",
    "In a random effects meta-analysis we assume the true effect (on a log-odds scale) $d_i$ in a trial $i$\n",
    "is drawn from some population distribution. Let $r^C_i$ denote number of events in the control group in trial $i$,\n",
    "and $r^T_i$ denote events under active treatment in trial $i$. Our model is:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "r^C_i &\\sim \\text{Binomial}\\left(p^C_i, n^C_i\\right) \\\\\n",
    "r^T_i &\\sim \\text{Binomial}\\left(p^T_i, n^T_i\\right) \\\\\n",
    "\\text{logit}\\left(p^C_i\\right) &= \\mu_i \\\\\n",
    "\\text{logit}\\left(p^T_i\\right) &= \\mu_i + \\delta_i \\\\\n",
    "\\delta_i &\\sim \\text{Normal}(d, t) \\\\\n",
    "\\mu_i &\\sim \\text{Normal}(m, s)\n",
    "\\end{aligned}$$\n",
    "\n",
    "We want to make inferences about the population effect $d$, and the predictive distribution for the effect $\\delta_{\\text{new}}$ in a new trial. Build a model to estimate these quantities in PyMC, and (1) use convergence diagnostics to check for convergence and (2) use posterior predictive checks to assess goodness-of-fit.\n",
    "\n",
    "Here are the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_t_obs = [3, 7, 5, 102, 28, 4, 98, 60, 25, 138, 64, 45, 9, 57, 25, 33, 28, 8, 6, 32, 27, 22]\n",
    "n_t_obs = [38, 114, 69, 1533, 355, 59, 945, 632, 278,1916, 873, 263, 291, 858, 154, 207, 251, 151, 174, 209, 391, 680]\n",
    "r_c_obs = [3, 14, 11, 127, 27, 6, 152, 48, 37, 188, 52, 47, 16, 45, 31, 38, 12, 6, 3, 40, 43, 39]\n",
    "n_c_obs = [39, 116, 93, 1520, 365, 52, 939, 471, 282, 1921, 583, 266, 293, 883, 147, 213, 122, 154, 134, 218, 364, 674]\n",
    "N = len(n_c_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Gelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. Statistical Science. A Review Journal of the Institute of Mathematical Statistics, 457–472.\n",
    "\n",
    "Geweke, J., Berger, J. O., & Dawid, A. P. (1992). Evaluating the accuracy of sampling-based approaches to the calculation of posterior moments. In Bayesian Statistics 4.\n",
    "\n",
    "Brooks, S. P., Catchpole, E. A., & Morgan, B. J. T. (2000). Bayesian Animal Survival Estimation. Statistical Science. A Review Journal of the Institute of Mathematical Statistics, 15(4), 357–376. doi:10.1214/ss/1177010123\n",
    "\n",
    "Gelman, A., Meng, X., & Stern, H. (1996). Posterior predicitive assessment of model fitness via realized discrepencies with discussion. Statistica Sinica, 6, 733–807.\n",
    "\n",
    "Raftery, A., & Lewis, S. (1992). One long run with diagnostics: Implementation strategies for Markov chain Monte Carlo. Statistical Science. A Review Journal of the Institute of Mathematical Statistics, 7, 493–497.\n",
    "\n",
    "[CrossValidated: How to use scikit-learn's cross validation functions on multi-label classifiers](http://stats.stackexchange.com/questions/65828/how-to-use-scikit-learns-cross-validation-functions-on-multi-label-classifiers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
