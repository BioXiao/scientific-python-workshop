{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models in PyMC\n",
    "\n",
    "Bayesian inference begins with specification of a probability model\n",
    "relating unknown variables to data. PyMC provides three basic building\n",
    "blocks for Bayesian probability models: `Stochastic`, `Deterministic`\n",
    "and `Potential`.\n",
    "\n",
    "A `Stochastic` object represents a variable whose value is not\n",
    "completely determined by its parents, and a `Deterministic` object\n",
    "represents a variable that is entirely determined by its parents. In\n",
    "object-oriented programming parlance, `Stochastic` and `Deterministic`\n",
    "are subclasses of the `Variable` class, which only serves as a template\n",
    "for other classes and is never actually implemented in models.\n",
    "\n",
    "The third basic class, `Potential`, represents 'factor potentials', which are *not* variables but simply\n",
    "log-likelihood terms and/or constraints that are multiplied into joint\n",
    "distributions to modify them. `Potential` and `Variable` are subclasses\n",
    "of `Node`.\n",
    "\n",
    "## The Stochastic class\n",
    "\n",
    "A stochastic variable has the following primary attributes:\n",
    "\n",
    "`value`\n",
    ":   The variable's current value.\n",
    "\n",
    "`logp`\n",
    ":   The log-probability of the variable's current value given the values\n",
    "    of its parents.\n",
    "\n",
    "A stochastic variable can optionally be endowed with a method called\n",
    "`random`, which draws a value for the variable given the values of its\n",
    "parents. \n",
    "\n",
    "### Creation of stochastic variables\n",
    "\n",
    "There are three main ways to create stochastic variables, called the\n",
    "**automatic**, **decorator**, and **direct** interfaces.\n",
    "\n",
    "**Automatic**\n",
    "\n",
    "Stochastic variables with standard distributions provided by PyMC can be created in a\n",
    "single line using special subclasses of `Stochastic`. For example, the uniformly-distributed discrete variable $switchpoint$ in the coal mining disasters model is created using the automatic interface as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "from pymc.examples import disaster_model\n",
    "\n",
    "switchpoint = pm.DiscreteUniform('switchpoint', lower=0, upper=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the rate parameters can automatically be given exponential priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "early_mean = pm.Exponential('early_mean', beta=1., value=1)\n",
    "late_mean = pm.Exponential('late_mean', beta=1., value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decorator**\n",
    "\n",
    "Uniformly-distributed discrete stochastic variable $switchpoint$ in the disasters model could alternatively be created from a function that computes its log-probability as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@pm.stochastic\n",
    "def switchpoint(value=1900, t_l=1851, t_h=1962):\n",
    "    \"\"\"The switchpoint for the rate of disaster occurrence.\"\"\"\n",
    "    if value > t_h or value < t_l:\n",
    "        # Invalid values\n",
    "        return -np.inf\n",
    "    else:\n",
    "        # Uniform log-likelihood\n",
    "        return -np.log(t_h - t_l + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a simple Python function preceded by a Python\n",
    "expression called a **decorator**, here called\n",
    "`@stochastic`. Generally, decorators enhance functions with\n",
    "additional properties or functionality. The `Stochastic` object\n",
    "produced by the `@stochastic` decorator will evaluate its\n",
    "log-probability using the function `switchpoint`. The `value`\n",
    "argument, which is required, provides an initial value for the\n",
    "variable. The remaining arguments will be assigned as parents of\n",
    "`switchpoint` (*i.e.* they will populate the `parents` dictionary).\n",
    "\n",
    "To emphasize, the Python function decorated by `@stochastic` should\n",
    "compute the *log*-density or *log*-probability of the variable. That\n",
    "is why the return value in the example above is $-\\log(t_h-t_l+1)$\n",
    "rather than $1/(t_h-t_l+1)$.\n",
    "\n",
    "**Direct**\n",
    "\n",
    "Its also possible to instantiate `Stochastic` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def switchpoint_logp(value, t_l, t_h):\n",
    "    if value > t_h or value < t_l:\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return -np.log(t_h - t_l + 1)\n",
    "\n",
    "def switchpoint_rand(t_l, t_h):\n",
    "    return np.round( (t_l - t_h) * np.random.random() ) + t_l\n",
    "\n",
    "switchpoint = pm.Stochastic( logp = switchpoint_logp,\n",
    "                doc = 'The switchpoint for the rate of disaster occurrence.',\n",
    "                name = 'switchpoint',\n",
    "                parents = {'t_l': 1851, 't_h': 1962},\n",
    "                random = switchpoint_rand,\n",
    "                trace = True,\n",
    "                value = 1900,\n",
    "                dtype=int,\n",
    "                rseed = 1.,\n",
    "                observed = False,\n",
    "                cache_depth = 2,\n",
    "                plot=True,\n",
    "                verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the log-probability and random variate functions are\n",
    "specified externally and passed to `Stochastic` as arguments. This\n",
    "is a rather awkward way to instantiate a stochastic variable;\n",
    "consequently, such implementations should be rare.\n",
    "\n",
    "## Data Stochastics\n",
    "\n",
    "Data are represented by `Stochastic` objects whose `observed` attribute\n",
    "is set to `True`. If a stochastic variable's `observed` flag is `True`,\n",
    "its value cannot be changed, and it won't be sampled by the fitting\n",
    "method.\n",
    "\n",
    "In each interface, an optional keyword argument `observed` can be set to\n",
    "`True`. In the decorator interface, the\n",
    "`@observed` decorator is used instead of `@stochastic`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import poisson\n",
    "\n",
    "@pm.observed\n",
    "def likelihood(value=[1, 2, 1, 5], parameter=3):\n",
    "    return poisson.logpmf(value, parameter).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the other interfaces, the `observed=True` argument is added to the\n",
    "instantiation of the `Stochastic`, or its subclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disasters = pm.Poisson('disasters', mu=2, \n",
    "                       value=disaster_model.disasters_array, \n",
    "                       observed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Deterministic class\n",
    "\n",
    "The `Deterministic` class represents variables whose values are\n",
    "completely determined by the values of their parents. For example, in\n",
    "our disasters model, $rate$ is a `deterministic` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@pm.deterministic\n",
    "def rate(s=switchpoint, e=early_mean, l=late_mean):\n",
    "    ''' Concatenate Poisson means '''\n",
    "    out = np.empty(len(disaster_model.disasters_array))\n",
    "    out[:s] = e\n",
    "    out[s:] = l\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so `rate`'s value can be computed exactly from the values of its parents\n",
    "`early_mean`, `late_mean` and `switchpoint`.\n",
    "\n",
    "A `Deterministic` variable's most important attribute is `value`, which\n",
    "gives the current value of the variable given the values of its parents.\n",
    "Like `Stochastic`'s `logp` attribute, this attribute is computed\n",
    "on-demand and cached for efficiency.\n",
    "\n",
    "A Deterministic variable has the following additional attributes:\n",
    "\n",
    "`parents`\n",
    ":   A dictionary containing the variable's parents. The keys of the dictionary correspond to the names assigned to the variable's parents by the variable, and the values correspond to the actual parents.\n",
    "\n",
    "`children`\n",
    ":   A set containing the variable's children, which must be nodes.\n",
    "\n",
    "Deterministic variables have no methods.\n",
    "\n",
    "### Creation of deterministic variables\n",
    "\n",
    "Deterministic variables are less complicated than stochastic variables,\n",
    "and have similar **automatic**, **decorator**, and **direct**\n",
    "interfaces:\n",
    "\n",
    "**Automatic**\n",
    "\n",
    "A handful of common functions have been wrapped in Deterministic\n",
    "objects. These are brief enough to list:\n",
    "\n",
    "`LinearCombination`\n",
    ":   Has two parents $x$ and $y$, both of which must be iterable (*i.e.* vector-valued). This function returns:\n",
    "\n",
    "\\\\[\\sum_i x_i^T y_i\\\\]\n",
    "\n",
    "`Index`\n",
    ":   Has two parents $x$ and `index`. $x$ must be iterable, `index` must be valued as an integer. \n",
    "\n",
    "\\\\[x[\\text{index}]\\\\]\n",
    "\n",
    "\n",
    "`Index` is useful for implementing dynamic models, in which the parent-child connections change.\n",
    "\n",
    "`Lambda`\n",
    ":   Converts an anonymous function (in Python, called **lambda functions**) to a `Deterministic` instance on a single line.\n",
    "\n",
    "`CompletedDirichlet`\n",
    ":   PyMC represents Dirichlet variables of length $k$ by the first $k-1$ elements; since they must sum to 1, the $k^{th}$ element is determined by the others. `CompletedDirichlet` appends the $k^{th}$ element to the value of its parent $D$.\n",
    "\n",
    "`Logit`, `InvLogit`, `StukelLogit`, `StukelInvLogit`\n",
    ":   Common link functions for generalized linear models, and their inverses.\n",
    "\n",
    "Its a good idea to use these classes when feasible in order to give hints to step methods.\n",
    "\n",
    "Certain elementary operations on variables create deterministic variables. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = pm.MvNormal('x', np.ones(3), np.eye(3))\n",
    "y = pm.MvNormal('y', np.ones(3), np.eye(3))\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x[0]+y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the objects thus created have `trace=False` and `plot=False` by default.\n",
    "\n",
    "**Decorator**\n",
    "\n",
    "We have seen in the disasters example how the decorator interface is used to create a deterministic variable. Notice that rather than returning the log-probability, as is the\n",
    "case for `Stochastic` objects, the function returns the value of the deterministic object, given its parents. Also notice that, unlike for `Stochastic` objects, there is no `value` argument\n",
    "passed, since the value is calculated deterministically by the\n",
    "function itself. \n",
    "\n",
    "\n",
    "**Direct**\n",
    "\n",
    "`Deterministic` objects can also be instantiated directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rate_eval(switchpoint=switchpoint, early_mean=early_mean, late_mean=late_mean):\n",
    "    value = np.zeros(111)\n",
    "    value[:switchpoint] = early_mean\n",
    "    value[switchpoint:] = late_mean\n",
    "    return value\n",
    "\n",
    "rate = pm.Deterministic(eval = rate_eval,\n",
    "                  name = 'rate',\n",
    "                  parents = {'switchpoint': switchpoint, \n",
    "                          'early_mean': early_mean, \n",
    "                          'late_mean': late_mean},\n",
    "                  doc = 'The rate of disaster occurrence.',\n",
    "                  trace = True,\n",
    "                  verbose = 0,\n",
    "                  dtype=float,\n",
    "                  plot=False,\n",
    "                  cache_depth = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containers\n",
    "\n",
    "In some situations it would be inconvenient to assign a unique label to\n",
    "each parent of some variable. Consider $y$ in the following model:\n",
    "\n",
    "$$\\begin{align*}\n",
    "x_0 &\\sim N (0,\\tau_x)\\\\\n",
    "x_{i+1}|x_i &\\sim \\text{N}(x_i, \\tau_x)\\\\\n",
    "&i=0,\\ldots, N-2\\\\\n",
    "y|x &\\sim N \\left(\\sum_{i=0}^{N-1}x_i^2,\\tau_y\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Here, $y$ depends on every element of the Markov chain $x$, but we\n",
    "wouldn't want to manually enter $N$ parent labels `x_0`,\n",
    "`x_1`, etc.\n",
    "\n",
    "This situation can be handled naturally in PyMC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "x_0 = pm.Normal('x_0', mu=0, tau=1)\n",
    "\n",
    "x = np.empty(N, dtype=object)\n",
    "x[0] = x_0\n",
    "\n",
    "for i in range(1, N):\n",
    "\n",
    "    x[i] = pm.Normal('x_%i' % i, mu=x[i-1], tau=1)\n",
    "\n",
    "@pm.observed\n",
    "def y(value=1, mu=x, tau=100):\n",
    "    return pm.normal_like(value, (mu**2).sum(), tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC automatically wraps array $x$ in an appropriate `Container` class.\n",
    "The expression `'x_%i' % i` labels each `Normal` object in the container\n",
    "with the appropriate index $i$. For example, if `i=1`, the name of the\n",
    "corresponding element becomes `x_1`.\n",
    "\n",
    "Containers, like variables, have an attribute called `value`. This\n",
    "attribute returns a copy of the (possibly nested) iterable that was\n",
    "passed into the container function, but with each variable inside\n",
    "replaced with its corresponding value.\n",
    "\n",
    "## The Potential class\n",
    "\n",
    "For some applications, we want to be able to modify the joint density by\n",
    "incorporating terms that don't correspond to probabilities of variables\n",
    "conditional on parents, for example:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "p(x_0, x_2, \\ldots x_{N-1}) \\propto \\prod_{i=0}^{N-2} \\psi_i(x_i, x_{i+1}).\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "In other cases we may want to add probability terms to existing models.\n",
    "For example, suppose we want to constrain the difference between the early and late means in the disaster model to be less than 1, so that the joint density becomes:\n",
    "\n",
    "$$p(y,\\tau,\\lambda_1,\\lambda_2) \\propto p(y|\\tau,\\lambda_1,\\lambda_2) p(\\tau) p(\\lambda_1) p(\\lambda_2) I(|\\lambda_2-\\lambda_1| \\lt 1)$$\n",
    "\n",
    "Arbitrary factors are implemented by objects of class `Potential`. Bayesian\n",
    "hierarchical notation doesn't accomodate these potentials. \n",
    "\n",
    "Potentials have one important attribute, `logp`, the log of their\n",
    "current probability or probability density value given the values of\n",
    "their parents. The only other additional attribute of interest is\n",
    "`parents`, a dictionary containing the potential's parents. Potentials\n",
    "have no methods. They have no `trace` attribute, because they are not\n",
    "variables. They cannot serve as parents of variables (for the same\n",
    "reason), so they have no `children` attribute.\n",
    "\n",
    "### Creation of Potentials\n",
    "\n",
    "There are two ways to create potentials:\n",
    "\n",
    "**Decorator**\n",
    "\n",
    "A potential can be created via a decorator in a way very similar to\n",
    "`Deterministic`'s decorator interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@pm.potential\n",
    "def rate_constraint(l1=early_mean, l2=late_mean):\n",
    "    if np.abs(l2 - l1) > 1:\n",
    "        return -np.inf\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function supplied should return the potential's current\n",
    "*log*-probability or *log*-density as a Numpy `float`. The\n",
    "`potential` decorator can take `verbose` and `cache_depth` arguments\n",
    "like the `stochastic` decorator.\n",
    "\n",
    "**Direct**\n",
    "\n",
    "The same potential could be created directly as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rate_constraint_logp(l1=early_mean, l2=late_mean):\n",
    "    if np.abs(l2 - l1) > 1:\n",
    "        return -np.inf\n",
    "    return 0\n",
    "\n",
    "rate_constraint = pm.Potential(logp = rate_constraint_logp,\n",
    "                    name = 'rate_constraint',\n",
    "                    parents = {'l1': early_mean, 'l2': late_mean},\n",
    "                    doc = 'Constraint on rate differences',\n",
    "                    verbose = 0,\n",
    "                    cache_depth = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Bioassay model\n",
    "\n",
    "Recall from a previous lecture the bioassay example, where the number of deaths in a toxicity experiment was modeled as a binomial response, with the probability of death being a linear function of dose:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y_i &\\sim \\text{Bin}(n_i, p_i) \\\\\n",
    "\\text{logit}(p_i) &= a + b x_i\n",
    "\\end{aligned}$$\n",
    "\n",
    "Implement this model in PyMC (we will show you how to fit the model later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Log dose in each group\n",
    "log_dose = [-.86, -.3, -.05, .73]\n",
    "\n",
    "# Sample size in each group\n",
    "n = 5\n",
    "\n",
    "# Outcomes\n",
    "deaths = [0, 1, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Models\n",
    "\n",
    "PyMC provides three objects that fit models:\n",
    "\n",
    "- `MCMC`, which coordinates Markov chain Monte Carlo algorithms. The actual work of updating stochastic variables conditional on the rest of the model is done by `StepMethod` objects.\n",
    "\n",
    "- `MAP`, which computes maximum *a posteriori* estimates.\n",
    "\n",
    "- `NormApprox`, the joint distribution of all stochastic variables in a model is approximated as normal using local information at the maximum *a posteriori* estimate.\n",
    "\n",
    "All three objects are subclasses of `Model`, which is PyMC's base class\n",
    "for fitting methods. `MCMC` and `NormApprox`, both of which can produce\n",
    "samples from the posterior, are subclasses of `Sampler`, which is PyMC's\n",
    "base class for Monte Carlo fitting methods. `Sampler` provides a generic\n",
    "sampling loop method and database support for storing large sets of\n",
    "joint samples. These base classes implement some basic methods that are\n",
    "inherited by the three implemented fitting methods, so they are\n",
    "documented at the end of this section.\n",
    "\n",
    "### Maximum a posteriori estimates\n",
    "\n",
    "The `MAP` class sets all stochastic variables to their maximum *a\n",
    "posteriori* values using functions in SciPy's `optimize` package; hence,\n",
    "SciPy must be installed to use it. `MAP` can only handle variables whose\n",
    "dtype is `float`, so it will not work, for example, on the disaster model example. \n",
    "\n",
    "We can fit the bioassay example using `MAP`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc.examples import gelman_bioassay\n",
    "M = pm.MAP(gelman_bioassay)\n",
    "M.fit(method='fmin_powell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This call will cause $M$ to fit the model using Powell's method, which does not require derivatives. The variables in `DisasterModel` have now been set to their maximum *a posteriori* values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M.alpha.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M.beta.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate model selection statistics, AIC and BIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M.AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M.BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MAP` has two useful methods:\n",
    "\n",
    "`fit(method ='fmin', iterlim=1000, tol=.0001)`\n",
    ":   The optimization method may be `fmin`, `fmin_l_bfgs_b`, `fmin_ncg`,\n",
    "    `fmin_cg`, or `fmin_powell`. See the documentation of SciPy's\n",
    "    `optimize` package for the details of these methods. The `tol` and\n",
    "    `iterlim` parameters are passed to the optimization function under\n",
    "    the appropriate names.\n",
    "\n",
    "`revert_to_max()`\n",
    ":   If the values of the constituent stochastic variables change after\n",
    "    fitting, this function will reset them to their maximum *a\n",
    "    posteriori* values.\n",
    "\n",
    "\n",
    "The useful attributes of `MAP` are:\n",
    "\n",
    "`logp`\n",
    ":   The joint log-probability of the model.\n",
    "\n",
    "`logp_at_max`\n",
    ":   The maximum joint log-probability of the model.\n",
    "\n",
    "`AIC`\n",
    ":   Akaike's information criterion for this model.\n",
    "\n",
    "`BIC`\n",
    ":   The Bayesian information criterion for this model.\n",
    "\n",
    "One use of the `MAP` class is finding reasonable initial states for MCMC\n",
    "chains. Note that multiple `Model` subclasses can handle the same\n",
    "collection of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal approximations\n",
    "\n",
    "The `NormApprox` class extends the `MAP` class by approximating the\n",
    "posterior covariance of the model using the Fisher information matrix,\n",
    "or the Hessian of the joint log probability at the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = pm.NormApprox(gelman_bioassay)\n",
    "N.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximate joint posterior mean and covariance of the variables are\n",
    "available via the attributes `mu` and `C`, which the the approximate posterior mean and variance/covariance, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N.mu[N.alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N.C[N.alpha, N.beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with `MAP`, the variables have been set to their maximum *a\n",
    "posteriori* values (which are also in the `mu` attribute) and the AIC\n",
    "and BIC of the model are available.\n",
    "\n",
    "We can also generate samples from the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N.sample(100)\n",
    "N.trace('alpha')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the methods and attributes of `MAP`, `NormApprox`\n",
    "provides the following methods:\n",
    "\n",
    "`sample(iter)`\n",
    ":   Samples from the approximate posterior distribution are drawn and stored.\n",
    "\n",
    "`isample(iter)`\n",
    ":   An 'interactive' version of `sample()`: sampling can be paused, returning control to the user.\n",
    "\n",
    "`draw`\n",
    ":   Sets all variables to random values drawn from the approximate posterior.\n",
    "    \n",
    "\n",
    "### MCMC\n",
    "\n",
    "The `MCMC` class implements PyMC's core business: producing Markov chain Monte Carlo samples for\n",
    "a model's variables. Its primary job is to create and coordinate a collection of 'step\n",
    "methods', each of which is responsible for updating one or more\n",
    "variables. \n",
    "\n",
    "`MCMC` provides the following useful methods:\n",
    "\n",
    "`sample(iter, burn, thin, tune_interval, tune_throughout, save_interval, ...)`\n",
    ":   Runs the MCMC algorithm and produces the traces. The `iter` argument\n",
    "controls the total number of MCMC iterations. No tallying will be\n",
    "done during the first `burn` iterations; these samples will be\n",
    "forgotten. After this burn-in period, tallying will be done each\n",
    "`thin` iterations. Tuning will be done each `tune_interval`\n",
    "iterations. If `tune_throughout=False`, no more tuning will be done\n",
    "after the burnin period. The model state will be saved every\n",
    "`save_interval` iterations, if given.\n",
    "\n",
    "`isample(iter, burn, thin, tune_interval, tune_throughout, save_interval, ...)`\n",
    ":   An interactive version of `sample`. The sampling loop may be paused\n",
    "at any time, returning control to the user.\n",
    "\n",
    "`use_step_method(method, *args, **kwargs)`:\n",
    ":   Creates an instance of step method class `method` to handle some\n",
    "stochastic variables. The extra arguments are passed to the `init`\n",
    "method of `method`. Assigning a step method to a variable manually\n",
    "will prevent the `MCMC` instance from automatically assigning one.\n",
    "However, you may handle a variable with multiple step methods.\n",
    "\n",
    "`stats()`:\n",
    ":   Generate summary statistics for all nodes in the model.\n",
    "\n",
    "The sampler's MCMC algorithms can be accessed via the `step_method_dict`\n",
    "attribute. `M.step_method_dict[x]` returns a list of the step methods\n",
    "`M` will use to handle the stochastic variable `x`.\n",
    "\n",
    "After sampling, the information tallied by `M` can be queried via\n",
    "`M.db.trace_names`. In addition to the values of variables, tuning\n",
    "information for adaptive step methods is generally tallied. These\n",
    "‘traces’ can be plotted to verify that tuning has in fact terminated. After sampling ends you can retrieve the trace as\n",
    "`M.trace[’var_name’]`.\n",
    "\n",
    "We can instantiate a MCMC sampler for the bioassay example as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = pm.MCMC(gelman_bioassay, db='sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step methods\n",
    "\n",
    "Step method objects handle individual stochastic variables, or sometimes groups \n",
    "of them. They are responsible for making the variables they handle take single \n",
    "MCMC steps conditional on the rest of the model. Each subclass of \n",
    "``StepMethod`` implements a method called ``step()``, which is called by \n",
    "``MCMC``. Step methods with adaptive tuning parameters can optionally implement \n",
    "a method called ``tune()``, which causes them to assess performance (based on \n",
    "the acceptance rates of proposed values for the variable) so far and adjust.\n",
    "\n",
    "The major subclasses of ``StepMethod`` are ``Metropolis`` and\n",
    "``AdaptiveMetropolis``. PyMC provides several flavors of the \n",
    "basic Metropolis steps.\n",
    "\n",
    "### Metropolis\n",
    "\n",
    "``Metropolis`` and subclasses implement Metropolis-Hastings steps. To tell an \n",
    "``MCMC`` object :math:`M` to handle a variable :math:`x` with a Metropolis step \n",
    "method, you might do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M.use_step_method(pm.Metropolis, M.alpha, proposal_sd=1., proposal_distribution='Normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Metropolis` itself handles float-valued variables, and subclasses\n",
    "`DiscreteMetropolis` and `BinaryMetropolis` handle integer- and\n",
    "boolean-valued variables, respectively.\n",
    "\n",
    "`Metropolis`' `__init__` method takes the following arguments:\n",
    "\n",
    "`stochastic`\n",
    ":   The variable to handle.\n",
    "\n",
    "`proposal_sd`\n",
    ":   A float or array of floats. This sets the proposal standard deviation if the proposal distribution is normal.\n",
    "\n",
    "`scale`\n",
    ":   A float, defaulting to 1. If the `scale` argument is provided but not `proposal_sd`, `proposal_sd` is computed as follows:\n",
    "\n",
    "```python\n",
    "if all(self.stochastic.value != 0.):\n",
    "    self.proposal_sd = (ones(shape(self.stochastic.value)) * \n",
    "                   abs(self.stochastic.value) * scale)\n",
    "else:\n",
    "    self.proposal_sd = ones(shape(self.stochastic.value)) * scale\n",
    "```\n",
    "\n",
    "`proposal_distribution`\n",
    ":   A string indicating which distribution should be used for proposals.\n",
    "Current options are `'Normal'` and `'Prior'`. If\n",
    "`proposal_distribution=None`, the proposal distribution is chosen\n",
    "automatically. It is set to `'Prior'` if the variable has no\n",
    "children and has a random method, and to `'Normal'` otherwise.\n",
    "\n",
    "Alhough the `proposal_sd` attribute is fixed at creation, Metropolis\n",
    "step methods adjust their initial proposal standard deviations using an\n",
    "attribute called `adaptive_scale_factor`. During tuning, the\n",
    "acceptance ratio of the step method is examined, and this scale factor\n",
    "is updated accordingly. If the proposal distribution is normal,\n",
    "proposals will have standard deviation\n",
    "`self.proposal_sd * self.adaptive_scale_factor`.\n",
    "\n",
    "By default, tuning will continue throughout the sampling loop, even\n",
    "after the burnin period is over. This can be changed via the\n",
    "`tune_throughout` argument to `MCMC.sample`. If an adaptive step\n",
    "method's `tally` flag is set (the default for `Metropolis`), a trace of\n",
    "its tuning parameters will be kept. If you allow tuning to continue\n",
    "throughout the sampling loop, it is important to verify that the\n",
    "'Diminishing Tuning' condition of [Roberts and Rosenthal (2007)](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.jap/1183667414) is satisfied: the\n",
    "amount of tuning should decrease to zero, or tuning should become very\n",
    "infrequent.\n",
    "\n",
    "If a Metropolis step method handles an array-valued variable, it\n",
    "proposes all elements independently but simultaneously. That is, it\n",
    "decides whether to accept or reject all elements together but it does\n",
    "not attempt to take the posterior correlation between elements into\n",
    "account. The `AdaptiveMetropolis` class (see below), on the other hand,\n",
    "does make correlated proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaptiveMetropolis\n",
    "\n",
    "The `AdaptativeMetropolis` (AM) step method works like a regular\n",
    "Metropolis step method, with the exception that its variables are\n",
    "block-updated using a multivariate jump distribution whose covariance is\n",
    "tuned during sampling. Although the chain is non-Markovian, it has\n",
    "correct ergodic properties ([Haario et al., 2001](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.bj/1080222083)).\n",
    "\n",
    "`AdaptiveMetropolis` works on vector-valued, continuous stochastics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc.examples import disaster_model_linear\n",
    "M = pm.MCMC(disaster_model_linear)\n",
    "M.use_step_method(pm.AdaptiveMetropolis, M.params_of_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AdaptativeMetropolis`'s init method takes the following arguments:\n",
    "\n",
    "`stochastics`\n",
    ":   The stochastic variables to handle. These will be updated jointly.\n",
    "\n",
    "`cov` (optional)\n",
    ":   An initial covariance matrix. Defaults to the identity matrix,\n",
    "adjusted according to the `scales` argument.\n",
    "\n",
    "`delay` (optional)\n",
    ":   The number of iterations to delay before computing the empirical\n",
    "covariance matrix.\n",
    "\n",
    "`scales` (optional):\n",
    ":   The initial covariance matrix will be diagonal, and its diagonal\n",
    "elements will be set to `scales` times the stochastics' values,\n",
    "squared.\n",
    "\n",
    "`interval` (optional):\n",
    ":   The number of iterations between updates of the covariance matrix.\n",
    "Defaults to 1000.\n",
    "\n",
    "`greedy` (optional):\n",
    ":   If `True`, only accepted jumps will be counted toward the delay\n",
    "before the covariance is first computed. Defaults to `True`.\n",
    "\n",
    "`shrink_if_necessary` (optional):\n",
    ":   Whether the proposal covariance should be shrunk if the acceptance\n",
    "rate becomes extremely small.\n",
    "\n",
    "In this algorithm, jumps are proposed from a multivariate normal\n",
    "distribution with covariance matrix $\\Sigma$. The algorithm first\n",
    "iterates until `delay` samples have been drawn (if `greedy` is true,\n",
    "until `delay` jumps have been accepted). At this point, $\\Sigma$ is\n",
    "given the value of the empirical covariance of the trace so far and\n",
    "sampling resumes. The covariance is then updated each `interval`\n",
    "iterations throughout the entire sampling run. It is this constant\n",
    "adaptation of the proposal distribution that makes the chain\n",
    "non-Markovian.\n",
    "\n",
    "### DiscreteMetropolis\n",
    "\n",
    "This class is just like `Metropolis`, but specialized to handle\n",
    "`Stochastic` instances with dtype `int`. The jump proposal distribution\n",
    "can either be `'Normal'`, `'Prior'` or `'Poisson'` (the default). In the\n",
    "normal case, the proposed value is drawn from a normal distribution\n",
    "centered at the current value and then rounded to the nearest integer.\n",
    "\n",
    "### BinaryMetropolis\n",
    "\n",
    "This class is specialized to handle `Stochastic` instances with dtype\n",
    "`bool`.\n",
    "\n",
    "For array-valued variables, `BinaryMetropolis` can be set to propose\n",
    "from the prior by passing in `dist=\"Prior\"`. Otherwise, the argument\n",
    "`p_jump` of the init method specifies how probable a change is. Like\n",
    "`Metropolis`' attribute `proposal_sd`, `p_jump` is tuned throughout the\n",
    "sampling loop via `adaptive_scale_factor`.\n",
    "\n",
    "### Automatic assignment of step methods\n",
    "\n",
    "Every step method subclass (including user-defined ones) that does not\n",
    "require any `__init__` arguments other than the stochastic variable to\n",
    "be handled adds itself to a list called `StepMethodRegistry` in the PyMC\n",
    "namespace. If a stochastic variable in an `MCMC` object has not been\n",
    "explicitly assigned a step method, each class in `StepMethodRegistry` is\n",
    "allowed to examine the variable.\n",
    "\n",
    "To do so, each step method implements a class method called\n",
    "`competence(stochastic)`, whose only argument is a single stochastic\n",
    "variable. These methods return values from 0 to 3; 0 meaning the step\n",
    "method cannot safely handle the variable and 3 meaning it will most\n",
    "likely perform well for variables like this. The `MCMC` object assigns\n",
    "the step method that returns the highest competence value to each of its\n",
    "stochastic variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running MCMC Samplers\n",
    "\n",
    "We can carry out Markov chain Monte Carlo sampling by calling the `sample` method (or in the terminal, `isample`) with the appropriate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = pm.MCMC(gelman_bioassay)\n",
    "M.sample(10000, burn=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pm.Matplot.plot(M.LD50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
